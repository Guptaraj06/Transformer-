Scaled Dot Product Self-Attention

Inside each attention head is a Scaled Dot Product Self-Attention operation . Given queries, keys, and values, the operation returns a new "mix" of the values.

Attention(Q,K,V)=softmax(QKT/sqrt(dk))V

